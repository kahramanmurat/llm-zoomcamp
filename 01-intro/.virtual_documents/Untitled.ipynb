from dotenv import load_dotenv
import openai
import os


from openai import OpenAI


# Load environment variables from the .env file
load_dotenv()


# Access the variables
api_key = os.getenv('OPENAI_API_KEY')


client = openai.OpenAI(api_key=api_key)





!curl localhost:9200








import requests 

docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'
docs_response = requests.get(docs_url)
documents_raw = docs_response.json()

documents = []

for course in documents_raw:
    course_name = course['course']

    for doc in course['documents']:
        doc['course'] = course_name
        documents.append(doc)


documents[0]





from elasticsearch import Elasticsearch


es_client=Elasticsearch('http://localhost:9200')


index_settings={
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
    },
    "mappings": {
        "properties": {
            "text": {"type": "text"},
            "section": {"type": "text"},
            "question": {"type": "text"},
            "course": {"type": "keyword"} 
        }
    }
}

index_name='homework-questions'

es_client.indices.create(index=index_name,body=index_settings)


for doc in documents:
    es_client.index(index=index_name,document=doc)








query='How do I execute a command in a running docker container?'


def elastic_search(query):
    search_query={
    "size": 5,
    "query": {
        "bool": {
            "must": {
                "multi_match": {
                    "query": query,
                    "fields": ["question^4", "text"],
                    "type": "best_fields"
                }
            }
        }
    }
    }
    response=es_client.search(index=index_name, body=search_query)

    top_score = response['hits']['hits'][0]['_score']
        
    return top_score


elastic_search(query)








def elastic_search(query):
    search_query={
    "size": 3,
    "query": {
        "bool": {
            "must": {
                "multi_match": {
                    "query": query,
                    "fields": ["question^4", "text"],
                    "type": "best_fields"
                }
            },
            "filter": {
                "term": {
                    "course": "machine-learning-zoomcamp"
                }
            }
        }
    }
    }
    response=es_client.search(index=index_name, body=search_query)
    
    result_docs=[]
    
    for response in response['hits']['hits']:
        result_docs.append(response['_source'])
        
    return result_docs


elastic_search(query)[2]['question']





records=elastic_search(query)


context_template = """
Q: {question}
A: {text}
""".strip()

prompt_template = """
You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.
Use only the facts from the CONTEXT when answering the QUESTION.

QUESTION: {question}

CONTEXT:
{context}
""".strip()

context = "\n\n".join([context_template.format(question=record['question'], text=record['text']) for record in records])

question = "How do I execute a command in a running docker container?"

prompt = prompt_template.format(question=question, context=context)

prompt_length = len(prompt)
print(prompt_length)









import tiktoken

# Initialize the encoding for GPT-4 model
encoding = tiktoken.encoding_for_model("gpt-4")

# Encode the prompt and count the number of tokens
tokens = encoding.encode(prompt)
num_tokens = len(tokens)

print(num_tokens)









def build_prompt(query,search_results):
    
    context_template = """
    Q: {question}
    A: {text}
    """.strip()
    
    prompt_template = """
    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.
    Use only the facts from the CONTEXT when answering the QUESTION.
    
    QUESTION: {question}
    
    CONTEXT:
    {context}
    """.strip()
    
    ccontext = "\n\n".join([context_template.format(question=record['question'], text=record['text']) for record in records])
    
    for doc in search_results:
        context=context + f"section: {doc['section']}\nquestion: {doc['question']}\nanswer: {doc['text']}\n\n"
        
    prompt=prompt_template.format(question=query,context=context).strip()
    return prompt


def llm(prompt):
    response=client.chat.completions.create(model='gpt-4o', messages=[{"role":"user","content":prompt}])
    return response.choices[0].message.content


def rag(query):
    search_results=elastic_search(query)
    prompt=build_prompt(query,search_results)
    answer=llm(prompt)
    return answer


answer=llm(prompt)


answer





# Constants
tokens_per_request_input = 150
tokens_per_request_output = 250
cost_per_1k_input_tokens = 0.005
cost_per_1k_output_tokens = 0.015
number_of_requests = 1000

# Calculate total tokens
total_input_tokens = tokens_per_request_input * number_of_requests
total_output_tokens = tokens_per_request_output * number_of_requests

# Convert tokens to thousands of tokens
total_input_tokens_in_k = total_input_tokens / 1000
total_output_tokens_in_k = total_output_tokens / 1000

# Calculate cost
cost_input = total_input_tokens_in_k * cost_per_1k_input_tokens
cost_output = total_output_tokens_in_k * cost_per_1k_output_tokens

# Total cost
total_cost = cost_input + cost_output

print(f"Total cost to run {number_of_requests} requests: ${total_cost:.2f}")




